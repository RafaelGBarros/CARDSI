{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWsGkxdGRhBzOBgay9AeZ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmanuelCAC/data_science_ads/blob/main/RedesNeurais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bibliotecas"
      ],
      "metadata": {
        "id": "89hP68XEHudg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C5D3PEFyHpfB"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ligando o Google Drive"
      ],
      "metadata": {
        "id": "iN-HyEOkH-Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "caminho = '/content/drive/MyDrive/Faculdade/6Semestre/CienciadeDados/MaterialApoio/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjHFjaX-IBaz",
        "outputId": "b27ab42b-1084-4d33-bf61-09c2e100289e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base - Credit Data"
      ],
      "metadata": {
        "id": "ydh-IkLqIRE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(caminho + 'credit.pkl', 'rb') as f:\n",
        "  X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)"
      ],
      "metadata": {
        "id": "0M-UU5n2IQdA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_credit_treinamento.shape, y_credit_treinamento.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRHCMl1fYmR-",
        "outputId": "6eb5e5d2-681b-496f-9934-cbd5d689452a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1500, 3), (1500,))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_credit_teste.shape, y_credit_teste.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaHvFN6PYoHV",
        "outputId": "be7783a1-9293-4469-fad4-66f720a6d1ae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((500, 3), (500,))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100,\n",
        "                                   solver='adam', activation='relu',\n",
        "                                   hidden_layer_sizes=(20,20))\n",
        "rede_neural_credit.fit(X_credit_treinamento, y_credit_treinamento)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3cLB5NoQYqVV",
        "outputId": "0ac678ad-5b2f-40ad-e699-70fa643bf909"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.76796322\n",
            "Iteration 2, loss = 0.67567904\n",
            "Iteration 3, loss = 0.60138308\n",
            "Iteration 4, loss = 0.54357537\n",
            "Iteration 5, loss = 0.49640847\n",
            "Iteration 6, loss = 0.45856403\n",
            "Iteration 7, loss = 0.42629570\n",
            "Iteration 8, loss = 0.39947386\n",
            "Iteration 9, loss = 0.37686808\n",
            "Iteration 10, loss = 0.35774654\n",
            "Iteration 11, loss = 0.34085731\n",
            "Iteration 12, loss = 0.32591219\n",
            "Iteration 13, loss = 0.31234499\n",
            "Iteration 14, loss = 0.30012922\n",
            "Iteration 15, loss = 0.28908091\n",
            "Iteration 16, loss = 0.27893663\n",
            "Iteration 17, loss = 0.26958915\n",
            "Iteration 18, loss = 0.26100049\n",
            "Iteration 19, loss = 0.25275805\n",
            "Iteration 20, loss = 0.24484970\n",
            "Iteration 21, loss = 0.23737432\n",
            "Iteration 22, loss = 0.23033811\n",
            "Iteration 23, loss = 0.22347318\n",
            "Iteration 24, loss = 0.21676455\n",
            "Iteration 25, loss = 0.21023500\n",
            "Iteration 26, loss = 0.20366566\n",
            "Iteration 27, loss = 0.19707330\n",
            "Iteration 28, loss = 0.19069681\n",
            "Iteration 29, loss = 0.18425464\n",
            "Iteration 30, loss = 0.17830612\n",
            "Iteration 31, loss = 0.17218707\n",
            "Iteration 32, loss = 0.16618717\n",
            "Iteration 33, loss = 0.16059974\n",
            "Iteration 34, loss = 0.15474061\n",
            "Iteration 35, loss = 0.14964559\n",
            "Iteration 36, loss = 0.14398379\n",
            "Iteration 37, loss = 0.13889423\n",
            "Iteration 38, loss = 0.13371098\n",
            "Iteration 39, loss = 0.12896313\n",
            "Iteration 40, loss = 0.12420810\n",
            "Iteration 41, loss = 0.11979290\n",
            "Iteration 42, loss = 0.11560822\n",
            "Iteration 43, loss = 0.11141344\n",
            "Iteration 44, loss = 0.10749582\n",
            "Iteration 45, loss = 0.10374723\n",
            "Iteration 46, loss = 0.10012367\n",
            "Iteration 47, loss = 0.09682201\n",
            "Iteration 48, loss = 0.09343168\n",
            "Iteration 49, loss = 0.09051757\n",
            "Iteration 50, loss = 0.08773188\n",
            "Iteration 51, loss = 0.08515474\n",
            "Iteration 52, loss = 0.08264351\n",
            "Iteration 53, loss = 0.08045960\n",
            "Iteration 54, loss = 0.07832659\n",
            "Iteration 55, loss = 0.07631484\n",
            "Iteration 56, loss = 0.07436191\n",
            "Iteration 57, loss = 0.07260410\n",
            "Iteration 58, loss = 0.07080457\n",
            "Iteration 59, loss = 0.06913760\n",
            "Iteration 60, loss = 0.06750366\n",
            "Iteration 61, loss = 0.06594515\n",
            "Iteration 62, loss = 0.06435570\n",
            "Iteration 63, loss = 0.06285123\n",
            "Iteration 64, loss = 0.06134832\n",
            "Iteration 65, loss = 0.05991948\n",
            "Iteration 66, loss = 0.05849472\n",
            "Iteration 67, loss = 0.05707118\n",
            "Iteration 68, loss = 0.05577563\n",
            "Iteration 69, loss = 0.05458068\n",
            "Iteration 70, loss = 0.05328564\n",
            "Iteration 71, loss = 0.05214328\n",
            "Iteration 72, loss = 0.05111454\n",
            "Iteration 73, loss = 0.05006466\n",
            "Iteration 74, loss = 0.04904027\n",
            "Iteration 75, loss = 0.04804578\n",
            "Iteration 76, loss = 0.04712088\n",
            "Iteration 77, loss = 0.04620137\n",
            "Iteration 78, loss = 0.04535529\n",
            "Iteration 79, loss = 0.04450995\n",
            "Iteration 80, loss = 0.04386709\n",
            "Iteration 81, loss = 0.04300196\n",
            "Iteration 82, loss = 0.04225000\n",
            "Iteration 83, loss = 0.04155234\n",
            "Iteration 84, loss = 0.04085973\n",
            "Iteration 85, loss = 0.04021933\n",
            "Iteration 86, loss = 0.03966034\n",
            "Iteration 87, loss = 0.03900848\n",
            "Iteration 88, loss = 0.03842397\n",
            "Iteration 89, loss = 0.03782249\n",
            "Iteration 90, loss = 0.03728658\n",
            "Iteration 91, loss = 0.03676600\n",
            "Iteration 92, loss = 0.03630871\n",
            "Iteration 93, loss = 0.03570367\n",
            "Iteration 94, loss = 0.03523435\n",
            "Iteration 95, loss = 0.03477891\n",
            "Iteration 96, loss = 0.03430899\n",
            "Iteration 97, loss = 0.03380858\n",
            "Iteration 98, loss = 0.03353464\n",
            "Iteration 99, loss = 0.03297066\n",
            "Iteration 100, loss = 0.03255648\n",
            "Iteration 101, loss = 0.03219932\n",
            "Iteration 102, loss = 0.03172266\n",
            "Iteration 103, loss = 0.03145562\n",
            "Iteration 104, loss = 0.03097488\n",
            "Iteration 105, loss = 0.03058447\n",
            "Iteration 106, loss = 0.03022668\n",
            "Iteration 107, loss = 0.02984159\n",
            "Iteration 108, loss = 0.02958363\n",
            "Iteration 109, loss = 0.02922055\n",
            "Iteration 110, loss = 0.02883888\n",
            "Iteration 111, loss = 0.02850551\n",
            "Iteration 112, loss = 0.02818654\n",
            "Iteration 113, loss = 0.02802721\n",
            "Iteration 114, loss = 0.02761722\n",
            "Iteration 115, loss = 0.02724129\n",
            "Iteration 116, loss = 0.02697564\n",
            "Iteration 117, loss = 0.02671240\n",
            "Iteration 118, loss = 0.02645981\n",
            "Iteration 119, loss = 0.02619948\n",
            "Iteration 120, loss = 0.02583811\n",
            "Iteration 121, loss = 0.02559153\n",
            "Iteration 122, loss = 0.02535205\n",
            "Iteration 123, loss = 0.02516532\n",
            "Iteration 124, loss = 0.02489624\n",
            "Iteration 125, loss = 0.02459577\n",
            "Iteration 126, loss = 0.02437114\n",
            "Iteration 127, loss = 0.02405433\n",
            "Iteration 128, loss = 0.02393122\n",
            "Iteration 129, loss = 0.02367126\n",
            "Iteration 130, loss = 0.02343866\n",
            "Iteration 131, loss = 0.02324628\n",
            "Iteration 132, loss = 0.02304016\n",
            "Iteration 133, loss = 0.02287735\n",
            "Iteration 134, loss = 0.02256312\n",
            "Iteration 135, loss = 0.02247521\n",
            "Iteration 136, loss = 0.02211709\n",
            "Iteration 137, loss = 0.02203632\n",
            "Iteration 138, loss = 0.02189542\n",
            "Iteration 139, loss = 0.02159405\n",
            "Iteration 140, loss = 0.02144991\n",
            "Iteration 141, loss = 0.02123009\n",
            "Iteration 142, loss = 0.02107749\n",
            "Iteration 143, loss = 0.02091939\n",
            "Iteration 144, loss = 0.02068469\n",
            "Iteration 145, loss = 0.02049992\n",
            "Iteration 146, loss = 0.02036368\n",
            "Iteration 147, loss = 0.02024095\n",
            "Iteration 148, loss = 0.02001020\n",
            "Iteration 149, loss = 0.01992126\n",
            "Iteration 150, loss = 0.01977257\n",
            "Iteration 151, loss = 0.01964013\n",
            "Iteration 152, loss = 0.01944584\n",
            "Iteration 153, loss = 0.01928482\n",
            "Iteration 154, loss = 0.01911480\n",
            "Iteration 155, loss = 0.01892630\n",
            "Iteration 156, loss = 0.01881227\n",
            "Iteration 157, loss = 0.01867099\n",
            "Iteration 158, loss = 0.01850769\n",
            "Iteration 159, loss = 0.01854819\n",
            "Iteration 160, loss = 0.01828307\n",
            "Iteration 161, loss = 0.01820233\n",
            "Iteration 162, loss = 0.01805305\n",
            "Iteration 163, loss = 0.01779033\n",
            "Iteration 164, loss = 0.01786675\n",
            "Iteration 165, loss = 0.01762520\n",
            "Iteration 166, loss = 0.01742950\n",
            "Iteration 167, loss = 0.01737377\n",
            "Iteration 168, loss = 0.01723761\n",
            "Iteration 169, loss = 0.01727427\n",
            "Iteration 170, loss = 0.01703686\n",
            "Iteration 171, loss = 0.01693574\n",
            "Iteration 172, loss = 0.01695857\n",
            "Iteration 173, loss = 0.01671967\n",
            "Iteration 174, loss = 0.01652366\n",
            "Iteration 175, loss = 0.01642023\n",
            "Iteration 176, loss = 0.01635233\n",
            "Iteration 177, loss = 0.01616599\n",
            "Iteration 178, loss = 0.01607178\n",
            "Iteration 179, loss = 0.01601073\n",
            "Iteration 180, loss = 0.01588754\n",
            "Iteration 181, loss = 0.01577902\n",
            "Iteration 182, loss = 0.01561413\n",
            "Iteration 183, loss = 0.01553584\n",
            "Iteration 184, loss = 0.01548628\n",
            "Iteration 185, loss = 0.01529576\n",
            "Iteration 186, loss = 0.01520591\n",
            "Iteration 187, loss = 0.01517128\n",
            "Iteration 188, loss = 0.01496365\n",
            "Iteration 189, loss = 0.01494655\n",
            "Iteration 190, loss = 0.01485214\n",
            "Iteration 191, loss = 0.01482959\n",
            "Iteration 192, loss = 0.01468710\n",
            "Iteration 193, loss = 0.01459327\n",
            "Iteration 194, loss = 0.01449399\n",
            "Iteration 195, loss = 0.01437074\n",
            "Iteration 196, loss = 0.01435824\n",
            "Iteration 197, loss = 0.01430409\n",
            "Iteration 198, loss = 0.01416006\n",
            "Iteration 199, loss = 0.01402954\n",
            "Iteration 200, loss = 0.01390151\n",
            "Iteration 201, loss = 0.01382600\n",
            "Iteration 202, loss = 0.01374508\n",
            "Iteration 203, loss = 0.01367511\n",
            "Iteration 204, loss = 0.01359680\n",
            "Iteration 205, loss = 0.01351029\n",
            "Iteration 206, loss = 0.01347992\n",
            "Iteration 207, loss = 0.01333058\n",
            "Iteration 208, loss = 0.01333377\n",
            "Iteration 209, loss = 0.01320256\n",
            "Iteration 210, loss = 0.01312480\n",
            "Iteration 211, loss = 0.01323178\n",
            "Iteration 212, loss = 0.01296677\n",
            "Iteration 213, loss = 0.01285336\n",
            "Iteration 214, loss = 0.01287302\n",
            "Iteration 215, loss = 0.01269436\n",
            "Iteration 216, loss = 0.01282953\n",
            "Iteration 217, loss = 0.01260656\n",
            "Iteration 218, loss = 0.01259376\n",
            "Iteration 219, loss = 0.01246714\n",
            "Iteration 220, loss = 0.01239404\n",
            "Iteration 221, loss = 0.01236452\n",
            "Iteration 222, loss = 0.01227664\n",
            "Iteration 223, loss = 0.01213950\n",
            "Iteration 224, loss = 0.01211802\n",
            "Iteration 225, loss = 0.01210609\n",
            "Iteration 226, loss = 0.01198461\n",
            "Iteration 227, loss = 0.01199296\n",
            "Iteration 228, loss = 0.01184408\n",
            "Iteration 229, loss = 0.01177040\n",
            "Iteration 230, loss = 0.01172557\n",
            "Iteration 231, loss = 0.01161233\n",
            "Iteration 232, loss = 0.01154067\n",
            "Iteration 233, loss = 0.01153294\n",
            "Iteration 234, loss = 0.01151569\n",
            "Iteration 235, loss = 0.01140854\n",
            "Iteration 236, loss = 0.01138599\n",
            "Iteration 237, loss = 0.01123078\n",
            "Iteration 238, loss = 0.01124081\n",
            "Iteration 239, loss = 0.01120184\n",
            "Iteration 240, loss = 0.01102101\n",
            "Iteration 241, loss = 0.01098494\n",
            "Iteration 242, loss = 0.01097385\n",
            "Iteration 243, loss = 0.01087015\n",
            "Iteration 244, loss = 0.01086759\n",
            "Iteration 245, loss = 0.01081288\n",
            "Iteration 246, loss = 0.01075683\n",
            "Iteration 247, loss = 0.01070744\n",
            "Iteration 248, loss = 0.01064444\n",
            "Iteration 249, loss = 0.01055121\n",
            "Iteration 250, loss = 0.01052636\n",
            "Iteration 251, loss = 0.01045386\n",
            "Iteration 252, loss = 0.01042551\n",
            "Iteration 253, loss = 0.01044173\n",
            "Iteration 254, loss = 0.01031626\n",
            "Iteration 255, loss = 0.01018240\n",
            "Iteration 256, loss = 0.01015396\n",
            "Iteration 257, loss = 0.01014756\n",
            "Iteration 258, loss = 0.01002012\n",
            "Iteration 259, loss = 0.01001851\n",
            "Iteration 260, loss = 0.01016979\n",
            "Iteration 261, loss = 0.00998163\n",
            "Iteration 262, loss = 0.01000958\n",
            "Iteration 263, loss = 0.00988303\n",
            "Iteration 264, loss = 0.00977719\n",
            "Iteration 265, loss = 0.00971278\n",
            "Iteration 266, loss = 0.00964204\n",
            "Iteration 267, loss = 0.00972104\n",
            "Iteration 268, loss = 0.00955161\n",
            "Iteration 269, loss = 0.00951181\n",
            "Iteration 270, loss = 0.00942872\n",
            "Iteration 271, loss = 0.00964278\n",
            "Iteration 272, loss = 0.00936757\n",
            "Iteration 273, loss = 0.00942729\n",
            "Iteration 274, loss = 0.00935824\n",
            "Iteration 275, loss = 0.00927594\n",
            "Iteration 276, loss = 0.00920038\n",
            "Iteration 277, loss = 0.00916060\n",
            "Iteration 278, loss = 0.00904838\n",
            "Iteration 279, loss = 0.00900510\n",
            "Iteration 280, loss = 0.00900370\n",
            "Iteration 281, loss = 0.00899694\n",
            "Iteration 282, loss = 0.00902913\n",
            "Iteration 283, loss = 0.00885308\n",
            "Iteration 284, loss = 0.00879937\n",
            "Iteration 285, loss = 0.00874633\n",
            "Iteration 286, loss = 0.00874034\n",
            "Iteration 287, loss = 0.00867151\n",
            "Iteration 288, loss = 0.00862774\n",
            "Iteration 289, loss = 0.00859493\n",
            "Iteration 290, loss = 0.00854978\n",
            "Iteration 291, loss = 0.00848469\n",
            "Iteration 292, loss = 0.00851278\n",
            "Iteration 293, loss = 0.00847592\n",
            "Iteration 294, loss = 0.00837837\n",
            "Iteration 295, loss = 0.00840022\n",
            "Iteration 296, loss = 0.00839811\n",
            "Iteration 297, loss = 0.00828458\n",
            "Iteration 298, loss = 0.00824668\n",
            "Iteration 299, loss = 0.00814307\n",
            "Iteration 300, loss = 0.00812865\n",
            "Iteration 301, loss = 0.00805534\n",
            "Iteration 302, loss = 0.00824856\n",
            "Iteration 303, loss = 0.00803689\n",
            "Iteration 304, loss = 0.00796054\n",
            "Iteration 305, loss = 0.00798948\n",
            "Iteration 306, loss = 0.00791232\n",
            "Iteration 307, loss = 0.00784162\n",
            "Iteration 308, loss = 0.00783227\n",
            "Iteration 309, loss = 0.00787724\n",
            "Iteration 310, loss = 0.00775876\n",
            "Iteration 311, loss = 0.00776204\n",
            "Iteration 312, loss = 0.00777435\n",
            "Iteration 313, loss = 0.00764842\n",
            "Iteration 314, loss = 0.00765039\n",
            "Iteration 315, loss = 0.00760052\n",
            "Iteration 316, loss = 0.00772304\n",
            "Iteration 317, loss = 0.00763948\n",
            "Iteration 318, loss = 0.00761915\n",
            "Iteration 319, loss = 0.00753010\n",
            "Iteration 320, loss = 0.00754854\n",
            "Iteration 321, loss = 0.00747058\n",
            "Iteration 322, loss = 0.00738144\n",
            "Iteration 323, loss = 0.00734576\n",
            "Iteration 324, loss = 0.00738416\n",
            "Iteration 325, loss = 0.00722533\n",
            "Iteration 326, loss = 0.00725976\n",
            "Iteration 327, loss = 0.00717481\n",
            "Iteration 328, loss = 0.00714327\n",
            "Iteration 329, loss = 0.00716613\n",
            "Iteration 330, loss = 0.00709705\n",
            "Iteration 331, loss = 0.00704916\n",
            "Iteration 332, loss = 0.00700102\n",
            "Iteration 333, loss = 0.00703084\n",
            "Iteration 334, loss = 0.00695851\n",
            "Iteration 335, loss = 0.00696094\n",
            "Iteration 336, loss = 0.00698762\n",
            "Iteration 337, loss = 0.00684990\n",
            "Iteration 338, loss = 0.00683538\n",
            "Iteration 339, loss = 0.00689796\n",
            "Iteration 340, loss = 0.00680970\n",
            "Iteration 341, loss = 0.00687658\n",
            "Iteration 342, loss = 0.00669512\n",
            "Iteration 343, loss = 0.00663107\n",
            "Iteration 344, loss = 0.00668085\n",
            "Iteration 345, loss = 0.00658316\n",
            "Iteration 346, loss = 0.00665490\n",
            "Iteration 347, loss = 0.00653339\n",
            "Iteration 348, loss = 0.00659024\n",
            "Iteration 349, loss = 0.00645902\n",
            "Iteration 350, loss = 0.00646033\n",
            "Iteration 351, loss = 0.00643866\n",
            "Iteration 352, loss = 0.00643303\n",
            "Iteration 353, loss = 0.00637397\n",
            "Iteration 354, loss = 0.00637140\n",
            "Iteration 355, loss = 0.00632806\n",
            "Iteration 356, loss = 0.00631066\n",
            "Iteration 357, loss = 0.00622612\n",
            "Iteration 358, loss = 0.00624356\n",
            "Iteration 359, loss = 0.00622616\n",
            "Iteration 360, loss = 0.00617974\n",
            "Iteration 361, loss = 0.00613442\n",
            "Iteration 362, loss = 0.00614156\n",
            "Iteration 363, loss = 0.00617846\n",
            "Iteration 364, loss = 0.00607667\n",
            "Iteration 365, loss = 0.00611513\n",
            "Iteration 366, loss = 0.00608542\n",
            "Iteration 367, loss = 0.00591764\n",
            "Iteration 368, loss = 0.00601128\n",
            "Iteration 369, loss = 0.00599907\n",
            "Iteration 370, loss = 0.00591906\n",
            "Iteration 371, loss = 0.00591596\n",
            "Iteration 372, loss = 0.00602785\n",
            "Iteration 373, loss = 0.00588549\n",
            "Iteration 374, loss = 0.00587062\n",
            "Iteration 375, loss = 0.00600742\n",
            "Iteration 376, loss = 0.00578618\n",
            "Iteration 377, loss = 0.00579559\n",
            "Iteration 378, loss = 0.00578455\n",
            "Iteration 379, loss = 0.00572305\n",
            "Iteration 380, loss = 0.00575387\n",
            "Iteration 381, loss = 0.00563186\n",
            "Iteration 382, loss = 0.00562102\n",
            "Iteration 383, loss = 0.00562425\n",
            "Iteration 384, loss = 0.00562629\n",
            "Iteration 385, loss = 0.00555901\n",
            "Iteration 386, loss = 0.00556833\n",
            "Iteration 387, loss = 0.00547723\n",
            "Iteration 388, loss = 0.00552641\n",
            "Iteration 389, loss = 0.00549286\n",
            "Iteration 390, loss = 0.00542591\n",
            "Iteration 391, loss = 0.00539788\n",
            "Iteration 392, loss = 0.00537630\n",
            "Iteration 393, loss = 0.00545157\n",
            "Iteration 394, loss = 0.00538531\n",
            "Iteration 395, loss = 0.00537748\n",
            "Iteration 396, loss = 0.00532547\n",
            "Iteration 397, loss = 0.00521268\n",
            "Iteration 398, loss = 0.00529777\n",
            "Iteration 399, loss = 0.00527206\n",
            "Iteration 400, loss = 0.00533789\n",
            "Iteration 401, loss = 0.00523457\n",
            "Iteration 402, loss = 0.00515653\n",
            "Iteration 403, loss = 0.00517140\n",
            "Iteration 404, loss = 0.00511919\n",
            "Iteration 405, loss = 0.00516276\n",
            "Iteration 406, loss = 0.00511227\n",
            "Iteration 407, loss = 0.00513400\n",
            "Iteration 408, loss = 0.00508054\n",
            "Iteration 409, loss = 0.00502968\n",
            "Iteration 410, loss = 0.00504675\n",
            "Iteration 411, loss = 0.00497626\n",
            "Iteration 412, loss = 0.00496436\n",
            "Iteration 413, loss = 0.00491919\n",
            "Iteration 414, loss = 0.00492335\n",
            "Iteration 415, loss = 0.00494365\n",
            "Iteration 416, loss = 0.00487260\n",
            "Iteration 417, loss = 0.00489128\n",
            "Iteration 418, loss = 0.00489821\n",
            "Iteration 419, loss = 0.00485119\n",
            "Iteration 420, loss = 0.00479884\n",
            "Iteration 421, loss = 0.00476951\n",
            "Iteration 422, loss = 0.00478138\n",
            "Iteration 423, loss = 0.00479978\n",
            "Iteration 424, loss = 0.00473310\n",
            "Iteration 425, loss = 0.00470723\n",
            "Iteration 426, loss = 0.00475810\n",
            "Iteration 427, loss = 0.00472890\n",
            "Iteration 428, loss = 0.00467675\n",
            "Iteration 429, loss = 0.00462628\n",
            "Iteration 430, loss = 0.00465087\n",
            "Iteration 431, loss = 0.00457765\n",
            "Iteration 432, loss = 0.00459207\n",
            "Iteration 433, loss = 0.00457291\n",
            "Iteration 434, loss = 0.00453380\n",
            "Iteration 435, loss = 0.00459657\n",
            "Iteration 436, loss = 0.00456149\n",
            "Iteration 437, loss = 0.00452883\n",
            "Iteration 438, loss = 0.00461626\n",
            "Iteration 439, loss = 0.00443932\n",
            "Iteration 440, loss = 0.00445925\n",
            "Iteration 441, loss = 0.00448800\n",
            "Iteration 442, loss = 0.00447153\n",
            "Iteration 443, loss = 0.00451615\n",
            "Iteration 444, loss = 0.00436627\n",
            "Iteration 445, loss = 0.00435104\n",
            "Iteration 446, loss = 0.00435104\n",
            "Iteration 447, loss = 0.00443798\n",
            "Iteration 448, loss = 0.00448639\n",
            "Iteration 449, loss = 0.00429870\n",
            "Iteration 450, loss = 0.00441370\n",
            "Iteration 451, loss = 0.00426991\n",
            "Iteration 452, loss = 0.00433770\n",
            "Iteration 453, loss = 0.00433297\n",
            "Iteration 454, loss = 0.00419311\n",
            "Iteration 455, loss = 0.00423964\n",
            "Iteration 456, loss = 0.00414699\n",
            "Iteration 457, loss = 0.00413459\n",
            "Iteration 458, loss = 0.00413680\n",
            "Iteration 459, loss = 0.00413147\n",
            "Iteration 460, loss = 0.00411525\n",
            "Iteration 461, loss = 0.00410052\n",
            "Iteration 462, loss = 0.00410270\n",
            "Iteration 463, loss = 0.00414862\n",
            "Iteration 464, loss = 0.00409681\n",
            "Iteration 465, loss = 0.00407248\n",
            "Iteration 466, loss = 0.00403301\n",
            "Iteration 467, loss = 0.00405126\n",
            "Iteration 468, loss = 0.00406520\n",
            "Iteration 469, loss = 0.00399854\n",
            "Iteration 470, loss = 0.00402042\n",
            "Iteration 471, loss = 0.00402139\n",
            "Iteration 472, loss = 0.00393319\n",
            "Iteration 473, loss = 0.00393759\n",
            "Iteration 474, loss = 0.00395393\n",
            "Iteration 475, loss = 0.00410908\n",
            "Iteration 476, loss = 0.00381641\n",
            "Iteration 477, loss = 0.00390795\n",
            "Iteration 478, loss = 0.00378017\n",
            "Iteration 479, loss = 0.00385345\n",
            "Iteration 480, loss = 0.00382801\n",
            "Iteration 481, loss = 0.00375671\n",
            "Iteration 482, loss = 0.00378279\n",
            "Iteration 483, loss = 0.00375282\n",
            "Iteration 484, loss = 0.00374855\n",
            "Iteration 485, loss = 0.00382989\n",
            "Iteration 486, loss = 0.00375941\n",
            "Iteration 487, loss = 0.00368730\n",
            "Iteration 488, loss = 0.00370676\n",
            "Iteration 489, loss = 0.00371517\n",
            "Iteration 490, loss = 0.00364281\n",
            "Iteration 491, loss = 0.00366195\n",
            "Iteration 492, loss = 0.00359723\n",
            "Iteration 493, loss = 0.00364427\n",
            "Iteration 494, loss = 0.00360572\n",
            "Iteration 495, loss = 0.00366953\n",
            "Iteration 496, loss = 0.00358092\n",
            "Iteration 497, loss = 0.00350333\n",
            "Iteration 498, loss = 0.00362221\n",
            "Iteration 499, loss = 0.00351984\n",
            "Iteration 500, loss = 0.00352401\n",
            "Iteration 501, loss = 0.00343586\n",
            "Iteration 502, loss = 0.00347170\n",
            "Iteration 503, loss = 0.00351163\n",
            "Iteration 504, loss = 0.00344206\n",
            "Iteration 505, loss = 0.00343016\n",
            "Iteration 506, loss = 0.00341561\n",
            "Iteration 507, loss = 0.00336892\n",
            "Iteration 508, loss = 0.00336816\n",
            "Iteration 509, loss = 0.00340013\n",
            "Iteration 510, loss = 0.00340467\n",
            "Iteration 511, loss = 0.00334350\n",
            "Iteration 512, loss = 0.00332114\n",
            "Iteration 513, loss = 0.00328967\n",
            "Iteration 514, loss = 0.00332026\n",
            "Iteration 515, loss = 0.00335555\n",
            "Iteration 516, loss = 0.00330376\n",
            "Iteration 517, loss = 0.00329479\n",
            "Iteration 518, loss = 0.00324197\n",
            "Iteration 519, loss = 0.00326298\n",
            "Iteration 520, loss = 0.00326919\n",
            "Iteration 521, loss = 0.00318960\n",
            "Iteration 522, loss = 0.00320469\n",
            "Iteration 523, loss = 0.00317006\n",
            "Iteration 524, loss = 0.00313841\n",
            "Iteration 525, loss = 0.00313324\n",
            "Iteration 526, loss = 0.00319579\n",
            "Iteration 527, loss = 0.00316111\n",
            "Iteration 528, loss = 0.00311252\n",
            "Iteration 529, loss = 0.00316590\n",
            "Iteration 530, loss = 0.00307062\n",
            "Iteration 531, loss = 0.00312950\n",
            "Iteration 532, loss = 0.00307564\n",
            "Iteration 533, loss = 0.00314100\n",
            "Iteration 534, loss = 0.00306706\n",
            "Iteration 535, loss = 0.00305125\n",
            "Iteration 536, loss = 0.00308072\n",
            "Iteration 537, loss = 0.00304663\n",
            "Iteration 538, loss = 0.00298714\n",
            "Iteration 539, loss = 0.00295721\n",
            "Iteration 540, loss = 0.00299066\n",
            "Iteration 541, loss = 0.00298814\n",
            "Iteration 542, loss = 0.00299032\n",
            "Iteration 543, loss = 0.00292250\n",
            "Iteration 544, loss = 0.00293334\n",
            "Iteration 545, loss = 0.00301804\n",
            "Iteration 546, loss = 0.00292987\n",
            "Iteration 547, loss = 0.00288737\n",
            "Iteration 548, loss = 0.00287977\n",
            "Iteration 549, loss = 0.00296343\n",
            "Iteration 550, loss = 0.00290899\n",
            "Iteration 551, loss = 0.00288814\n",
            "Iteration 552, loss = 0.00290738\n",
            "Iteration 553, loss = 0.00283159\n",
            "Iteration 554, loss = 0.00289603\n",
            "Iteration 555, loss = 0.00285404\n",
            "Iteration 556, loss = 0.00281765\n",
            "Iteration 557, loss = 0.00282292\n",
            "Iteration 558, loss = 0.00279172\n",
            "Iteration 559, loss = 0.00285006\n",
            "Iteration 560, loss = 0.00273544\n",
            "Iteration 561, loss = 0.00271067\n",
            "Iteration 562, loss = 0.00278628\n",
            "Iteration 563, loss = 0.00274255\n",
            "Iteration 564, loss = 0.00281755\n",
            "Iteration 565, loss = 0.00271954\n",
            "Iteration 566, loss = 0.00271678\n",
            "Iteration 567, loss = 0.00269695\n",
            "Iteration 568, loss = 0.00267789\n",
            "Iteration 569, loss = 0.00260207\n",
            "Iteration 570, loss = 0.00280283\n",
            "Iteration 571, loss = 0.00264011\n",
            "Iteration 572, loss = 0.00264100\n",
            "Iteration 573, loss = 0.00268935\n",
            "Iteration 574, loss = 0.00273499\n",
            "Iteration 575, loss = 0.00273726\n",
            "Iteration 576, loss = 0.00262796\n",
            "Iteration 577, loss = 0.00256454\n",
            "Iteration 578, loss = 0.00261439\n",
            "Iteration 579, loss = 0.00256908\n",
            "Iteration 580, loss = 0.00256497\n",
            "Iteration 581, loss = 0.00254306\n",
            "Iteration 582, loss = 0.00250982\n",
            "Iteration 583, loss = 0.00254254\n",
            "Iteration 584, loss = 0.00251444\n",
            "Iteration 585, loss = 0.00250515\n",
            "Iteration 586, loss = 0.00254673\n",
            "Iteration 587, loss = 0.00249624\n",
            "Iteration 588, loss = 0.00251021\n",
            "Iteration 589, loss = 0.00247960\n",
            "Iteration 590, loss = 0.00244341\n",
            "Iteration 591, loss = 0.00246981\n",
            "Iteration 592, loss = 0.00243104\n",
            "Iteration 593, loss = 0.00242982\n",
            "Iteration 594, loss = 0.00245512\n",
            "Iteration 595, loss = 0.00247034\n",
            "Iteration 596, loss = 0.00240092\n",
            "Iteration 597, loss = 0.00241130\n",
            "Iteration 598, loss = 0.00239598\n",
            "Iteration 599, loss = 0.00237648\n",
            "Iteration 600, loss = 0.00241263\n",
            "Iteration 601, loss = 0.00237421\n",
            "Iteration 602, loss = 0.00237442\n",
            "Iteration 603, loss = 0.00240577\n",
            "Iteration 604, loss = 0.00233363\n",
            "Iteration 605, loss = 0.00233251\n",
            "Iteration 606, loss = 0.00238534\n",
            "Iteration 607, loss = 0.00237751\n",
            "Iteration 608, loss = 0.00232769\n",
            "Iteration 609, loss = 0.00232942\n",
            "Iteration 610, loss = 0.00228232\n",
            "Iteration 611, loss = 0.00245348\n",
            "Iteration 612, loss = 0.00224676\n",
            "Iteration 613, loss = 0.00229938\n",
            "Iteration 614, loss = 0.00227219\n",
            "Iteration 615, loss = 0.00224948\n",
            "Iteration 616, loss = 0.00223663\n",
            "Iteration 617, loss = 0.00227869\n",
            "Iteration 618, loss = 0.00223470\n",
            "Iteration 619, loss = 0.00222596\n",
            "Iteration 620, loss = 0.00220732\n",
            "Iteration 621, loss = 0.00219517\n",
            "Iteration 622, loss = 0.00226103\n",
            "Iteration 623, loss = 0.00220700\n",
            "Iteration 624, loss = 0.00228817\n",
            "Iteration 625, loss = 0.00223288\n",
            "Iteration 626, loss = 0.00218067\n",
            "Iteration 627, loss = 0.00216873\n",
            "Iteration 628, loss = 0.00220368\n",
            "Iteration 629, loss = 0.00218436\n",
            "Iteration 630, loss = 0.00210681\n",
            "Iteration 631, loss = 0.00211898\n",
            "Iteration 632, loss = 0.00210122\n",
            "Iteration 633, loss = 0.00209291\n",
            "Iteration 634, loss = 0.00208179\n",
            "Iteration 635, loss = 0.00211302\n",
            "Iteration 636, loss = 0.00210529\n",
            "Iteration 637, loss = 0.00202204\n",
            "Iteration 638, loss = 0.00213240\n",
            "Iteration 639, loss = 0.00203436\n",
            "Iteration 640, loss = 0.00219958\n",
            "Iteration 641, loss = 0.00214781\n",
            "Iteration 642, loss = 0.00215117\n",
            "Iteration 643, loss = 0.00194562\n",
            "Iteration 644, loss = 0.00227008\n",
            "Iteration 645, loss = 0.00196903\n",
            "Iteration 646, loss = 0.00196816\n",
            "Iteration 647, loss = 0.00210517\n",
            "Iteration 648, loss = 0.00201938\n",
            "Iteration 649, loss = 0.00196197\n",
            "Iteration 650, loss = 0.00196647\n",
            "Iteration 651, loss = 0.00195995\n",
            "Iteration 652, loss = 0.00196445\n",
            "Iteration 653, loss = 0.00195684\n",
            "Iteration 654, loss = 0.00193964\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(20, 20), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-9 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-9 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-9 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-9 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-9 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-9 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-9 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-9 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-9 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-9 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-9 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-9 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-9 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-9 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(20, 20), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(20, 20), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsao_credit = rede_neural_credit.predict(X_credit_teste)\n",
        "previsao_credit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l29oXYKYYq-e",
        "outputId": "8e78fd7b-ae57-44b7-a8f8-b0f923be91b8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_credit_teste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpdeN9y_hEYr",
        "outputId": "3f05f54a-af12-4d97-b5c8-08e00e13eec4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_credit_teste, previsao_credit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDMLtfL_hGfY",
        "outputId": "a6817832-0182-437d-c16c-6fc2862ff738"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.996"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = ConfusionMatrix(rede_neural_credit)\n",
        "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
        "cm.score(X_credit_teste, y_credit_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "AVxoPU80hMky",
        "outputId": "4d970021-20e0-4716-b79b-2ddf2317832f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.996"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_credit_teste, previsao_credit))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssSMr_0lhTtK",
        "outputId": "b6c87ca8-2506-461c-c6ab-ce8f29be4647"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       436\n",
            "           1       0.98      0.97      0.98        64\n",
            "\n",
            "    accuracy                           0.99       500\n",
            "   macro avg       0.99      0.98      0.99       500\n",
            "weighted avg       0.99      0.99      0.99       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base - Census"
      ],
      "metadata": {
        "id": "fQ5Bn8jdhgM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(caminho + 'census.pkl', 'rb') as f:\n",
        "  X_census_treinamento, y_census_treinamento, X_census_teste, y_census_teste = pickle.load(f)"
      ],
      "metadata": {
        "id": "GFHyOsjbhcjT"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_census_treinamento.shape, y_census_treinamento.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7J-yAuliabE",
        "outputId": "4a6a11b2-9457-4b2b-d8ad-aa37ef1115c8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((27676, 108), (27676,))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_census_teste.shape, y_census_teste.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFykFJCyieYa",
        "outputId": "dc75a024-86d8-4802-881a-c2160f552aba"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4885, 108), (4885,))"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rede_neural_census = MLPClassifier(max_iter=1000, verbose=True, tol=0.0000100,\n",
        "                                   hidden_layer_sizes=(55,55))\n",
        "rede_neural_census.fit(X_census_treinamento, y_census_treinamento)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_50wik7Sif6C",
        "outputId": "15cd2007-844c-43a6-bab1-61f49c69122a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.38908146\n",
            "Iteration 2, loss = 0.32526451\n",
            "Iteration 3, loss = 0.31442140\n",
            "Iteration 4, loss = 0.30805033\n",
            "Iteration 5, loss = 0.30329615\n",
            "Iteration 6, loss = 0.29967993\n",
            "Iteration 7, loss = 0.29695965\n",
            "Iteration 8, loss = 0.29393635\n",
            "Iteration 9, loss = 0.29271491\n",
            "Iteration 10, loss = 0.29009212\n",
            "Iteration 11, loss = 0.28721526\n",
            "Iteration 12, loss = 0.28540565\n",
            "Iteration 13, loss = 0.28364012\n",
            "Iteration 14, loss = 0.28121137\n",
            "Iteration 15, loss = 0.27854634\n",
            "Iteration 16, loss = 0.27756473\n",
            "Iteration 17, loss = 0.27497681\n",
            "Iteration 18, loss = 0.27376708\n",
            "Iteration 19, loss = 0.27158326\n",
            "Iteration 20, loss = 0.27029505\n",
            "Iteration 21, loss = 0.26832035\n",
            "Iteration 22, loss = 0.26684158\n",
            "Iteration 23, loss = 0.26589295\n",
            "Iteration 24, loss = 0.26351040\n",
            "Iteration 25, loss = 0.26193769\n",
            "Iteration 26, loss = 0.26059174\n",
            "Iteration 27, loss = 0.25875955\n",
            "Iteration 28, loss = 0.25698503\n",
            "Iteration 29, loss = 0.25623831\n",
            "Iteration 30, loss = 0.25501793\n",
            "Iteration 31, loss = 0.25352224\n",
            "Iteration 32, loss = 0.25267329\n",
            "Iteration 33, loss = 0.25030215\n",
            "Iteration 34, loss = 0.25018287\n",
            "Iteration 35, loss = 0.24921191\n",
            "Iteration 36, loss = 0.24692262\n",
            "Iteration 37, loss = 0.24624777\n",
            "Iteration 38, loss = 0.24462567\n",
            "Iteration 39, loss = 0.24352766\n",
            "Iteration 40, loss = 0.24231405\n",
            "Iteration 41, loss = 0.24057725\n",
            "Iteration 42, loss = 0.23970503\n",
            "Iteration 43, loss = 0.23907665\n",
            "Iteration 44, loss = 0.23774533\n",
            "Iteration 45, loss = 0.23683671\n",
            "Iteration 46, loss = 0.23652371\n",
            "Iteration 47, loss = 0.23436363\n",
            "Iteration 48, loss = 0.23398069\n",
            "Iteration 49, loss = 0.23243845\n",
            "Iteration 50, loss = 0.23103071\n",
            "Iteration 51, loss = 0.23037682\n",
            "Iteration 52, loss = 0.22954086\n",
            "Iteration 53, loss = 0.22905585\n",
            "Iteration 54, loss = 0.22719290\n",
            "Iteration 55, loss = 0.22704403\n",
            "Iteration 56, loss = 0.22502006\n",
            "Iteration 57, loss = 0.22502833\n",
            "Iteration 58, loss = 0.22443432\n",
            "Iteration 59, loss = 0.22415889\n",
            "Iteration 60, loss = 0.22202324\n",
            "Iteration 61, loss = 0.22081873\n",
            "Iteration 62, loss = 0.22046485\n",
            "Iteration 63, loss = 0.21954440\n",
            "Iteration 64, loss = 0.21787932\n",
            "Iteration 65, loss = 0.21784396\n",
            "Iteration 66, loss = 0.21723326\n",
            "Iteration 67, loss = 0.21643794\n",
            "Iteration 68, loss = 0.21545048\n",
            "Iteration 69, loss = 0.21478507\n",
            "Iteration 70, loss = 0.21342479\n",
            "Iteration 71, loss = 0.21295103\n",
            "Iteration 72, loss = 0.21301700\n",
            "Iteration 73, loss = 0.21158596\n",
            "Iteration 74, loss = 0.21263977\n",
            "Iteration 75, loss = 0.21098401\n",
            "Iteration 76, loss = 0.20998691\n",
            "Iteration 77, loss = 0.20962281\n",
            "Iteration 78, loss = 0.20904685\n",
            "Iteration 79, loss = 0.20850421\n",
            "Iteration 80, loss = 0.20829050\n",
            "Iteration 81, loss = 0.20694562\n",
            "Iteration 82, loss = 0.20492509\n",
            "Iteration 83, loss = 0.20631236\n",
            "Iteration 84, loss = 0.20618174\n",
            "Iteration 85, loss = 0.20423145\n",
            "Iteration 86, loss = 0.20303990\n",
            "Iteration 87, loss = 0.20313235\n",
            "Iteration 88, loss = 0.20154434\n",
            "Iteration 89, loss = 0.20243797\n",
            "Iteration 90, loss = 0.20121019\n",
            "Iteration 91, loss = 0.20013535\n",
            "Iteration 92, loss = 0.19930104\n",
            "Iteration 93, loss = 0.19959866\n",
            "Iteration 94, loss = 0.19875369\n",
            "Iteration 95, loss = 0.19984770\n",
            "Iteration 96, loss = 0.19896570\n",
            "Iteration 97, loss = 0.19801078\n",
            "Iteration 98, loss = 0.19822221\n",
            "Iteration 99, loss = 0.19661006\n",
            "Iteration 100, loss = 0.19618331\n",
            "Iteration 101, loss = 0.19535818\n",
            "Iteration 102, loss = 0.19467992\n",
            "Iteration 103, loss = 0.19422525\n",
            "Iteration 104, loss = 0.19470365\n",
            "Iteration 105, loss = 0.19331022\n",
            "Iteration 106, loss = 0.19276642\n",
            "Iteration 107, loss = 0.19212735\n",
            "Iteration 108, loss = 0.19201264\n",
            "Iteration 109, loss = 0.19112166\n",
            "Iteration 110, loss = 0.19087215\n",
            "Iteration 111, loss = 0.19073994\n",
            "Iteration 112, loss = 0.18955716\n",
            "Iteration 113, loss = 0.19045823\n",
            "Iteration 114, loss = 0.19054710\n",
            "Iteration 115, loss = 0.18931695\n",
            "Iteration 116, loss = 0.19033485\n",
            "Iteration 117, loss = 0.18856016\n",
            "Iteration 118, loss = 0.18830635\n",
            "Iteration 119, loss = 0.18769908\n",
            "Iteration 120, loss = 0.18749665\n",
            "Iteration 121, loss = 0.18682759\n",
            "Iteration 122, loss = 0.18615554\n",
            "Iteration 123, loss = 0.18630625\n",
            "Iteration 124, loss = 0.18493263\n",
            "Iteration 125, loss = 0.18763998\n",
            "Iteration 126, loss = 0.18595922\n",
            "Iteration 127, loss = 0.18441804\n",
            "Iteration 128, loss = 0.18425787\n",
            "Iteration 129, loss = 0.18295030\n",
            "Iteration 130, loss = 0.18438319\n",
            "Iteration 131, loss = 0.18387973\n",
            "Iteration 132, loss = 0.18361987\n",
            "Iteration 133, loss = 0.18113931\n",
            "Iteration 134, loss = 0.18234700\n",
            "Iteration 135, loss = 0.18258861\n",
            "Iteration 136, loss = 0.18111838\n",
            "Iteration 137, loss = 0.18251389\n",
            "Iteration 138, loss = 0.17988481\n",
            "Iteration 139, loss = 0.18133263\n",
            "Iteration 140, loss = 0.17976296\n",
            "Iteration 141, loss = 0.17989291\n",
            "Iteration 142, loss = 0.17918916\n",
            "Iteration 143, loss = 0.17871702\n",
            "Iteration 144, loss = 0.17879247\n",
            "Iteration 145, loss = 0.17847448\n",
            "Iteration 146, loss = 0.17685351\n",
            "Iteration 147, loss = 0.17726833\n",
            "Iteration 148, loss = 0.17759951\n",
            "Iteration 149, loss = 0.17750632\n",
            "Iteration 150, loss = 0.17584436\n",
            "Iteration 151, loss = 0.17544178\n",
            "Iteration 152, loss = 0.17624908\n",
            "Iteration 153, loss = 0.17459366\n",
            "Iteration 154, loss = 0.17499158\n",
            "Iteration 155, loss = 0.17488574\n",
            "Iteration 156, loss = 0.17488062\n",
            "Iteration 157, loss = 0.17434073\n",
            "Iteration 158, loss = 0.17429739\n",
            "Iteration 159, loss = 0.17371184\n",
            "Iteration 160, loss = 0.17380053\n",
            "Iteration 161, loss = 0.17255860\n",
            "Iteration 162, loss = 0.17272284\n",
            "Iteration 163, loss = 0.17202493\n",
            "Iteration 164, loss = 0.17170026\n",
            "Iteration 165, loss = 0.17192837\n",
            "Iteration 166, loss = 0.17105884\n",
            "Iteration 167, loss = 0.17070425\n",
            "Iteration 168, loss = 0.17018801\n",
            "Iteration 169, loss = 0.16917151\n",
            "Iteration 170, loss = 0.17046205\n",
            "Iteration 171, loss = 0.17126841\n",
            "Iteration 172, loss = 0.16979495\n",
            "Iteration 173, loss = 0.16905897\n",
            "Iteration 174, loss = 0.16916863\n",
            "Iteration 175, loss = 0.16750150\n",
            "Iteration 176, loss = 0.16805240\n",
            "Iteration 177, loss = 0.16568111\n",
            "Iteration 178, loss = 0.16690183\n",
            "Iteration 179, loss = 0.16740729\n",
            "Iteration 180, loss = 0.16716056\n",
            "Iteration 181, loss = 0.16667012\n",
            "Iteration 182, loss = 0.16711403\n",
            "Iteration 183, loss = 0.16602165\n",
            "Iteration 184, loss = 0.16586951\n",
            "Iteration 185, loss = 0.16576890\n",
            "Iteration 186, loss = 0.16568132\n",
            "Iteration 187, loss = 0.16394892\n",
            "Iteration 188, loss = 0.16518428\n",
            "Iteration 189, loss = 0.16343965\n",
            "Iteration 190, loss = 0.16376283\n",
            "Iteration 191, loss = 0.16313281\n",
            "Iteration 192, loss = 0.16359317\n",
            "Iteration 193, loss = 0.16224344\n",
            "Iteration 194, loss = 0.16226143\n",
            "Iteration 195, loss = 0.16460532\n",
            "Iteration 196, loss = 0.16323199\n",
            "Iteration 197, loss = 0.16229896\n",
            "Iteration 198, loss = 0.16279678\n",
            "Iteration 199, loss = 0.16313736\n",
            "Iteration 200, loss = 0.16123741\n",
            "Iteration 201, loss = 0.16378466\n",
            "Iteration 202, loss = 0.16201699\n",
            "Iteration 203, loss = 0.15941294\n",
            "Iteration 204, loss = 0.16123109\n",
            "Iteration 205, loss = 0.16082482\n",
            "Iteration 206, loss = 0.16021251\n",
            "Iteration 207, loss = 0.16020788\n",
            "Iteration 208, loss = 0.15864384\n",
            "Iteration 209, loss = 0.15853512\n",
            "Iteration 210, loss = 0.15905268\n",
            "Iteration 211, loss = 0.15790710\n",
            "Iteration 212, loss = 0.15816229\n",
            "Iteration 213, loss = 0.15919557\n",
            "Iteration 214, loss = 0.15853580\n",
            "Iteration 215, loss = 0.15757564\n",
            "Iteration 216, loss = 0.15838747\n",
            "Iteration 217, loss = 0.15684288\n",
            "Iteration 218, loss = 0.15668956\n",
            "Iteration 219, loss = 0.15768701\n",
            "Iteration 220, loss = 0.15609633\n",
            "Iteration 221, loss = 0.15563302\n",
            "Iteration 222, loss = 0.15613534\n",
            "Iteration 223, loss = 0.15561447\n",
            "Iteration 224, loss = 0.15640537\n",
            "Iteration 225, loss = 0.15483975\n",
            "Iteration 226, loss = 0.15475066\n",
            "Iteration 227, loss = 0.15476482\n",
            "Iteration 228, loss = 0.15519672\n",
            "Iteration 229, loss = 0.15615112\n",
            "Iteration 230, loss = 0.15680525\n",
            "Iteration 231, loss = 0.15600736\n",
            "Iteration 232, loss = 0.15425566\n",
            "Iteration 233, loss = 0.15434599\n",
            "Iteration 234, loss = 0.15427580\n",
            "Iteration 235, loss = 0.15376251\n",
            "Iteration 236, loss = 0.15376114\n",
            "Iteration 237, loss = 0.15334215\n",
            "Iteration 238, loss = 0.15209176\n",
            "Iteration 239, loss = 0.15461142\n",
            "Iteration 240, loss = 0.15291105\n",
            "Iteration 241, loss = 0.15188594\n",
            "Iteration 242, loss = 0.15190981\n",
            "Iteration 243, loss = 0.15267914\n",
            "Iteration 244, loss = 0.15010846\n",
            "Iteration 245, loss = 0.15069885\n",
            "Iteration 246, loss = 0.15079730\n",
            "Iteration 247, loss = 0.15057080\n",
            "Iteration 248, loss = 0.15202148\n",
            "Iteration 249, loss = 0.15072398\n",
            "Iteration 250, loss = 0.15271037\n",
            "Iteration 251, loss = 0.15200904\n",
            "Iteration 252, loss = 0.14960331\n",
            "Iteration 253, loss = 0.15189949\n",
            "Iteration 254, loss = 0.15045356\n",
            "Iteration 255, loss = 0.14856086\n",
            "Iteration 256, loss = 0.14877453\n",
            "Iteration 257, loss = 0.14829132\n",
            "Iteration 258, loss = 0.15045275\n",
            "Iteration 259, loss = 0.14829761\n",
            "Iteration 260, loss = 0.14905790\n",
            "Iteration 261, loss = 0.14775972\n",
            "Iteration 262, loss = 0.14866889\n",
            "Iteration 263, loss = 0.14812073\n",
            "Iteration 264, loss = 0.14786161\n",
            "Iteration 265, loss = 0.14789181\n",
            "Iteration 266, loss = 0.14706256\n",
            "Iteration 267, loss = 0.14856611\n",
            "Iteration 268, loss = 0.14748449\n",
            "Iteration 269, loss = 0.14634359\n",
            "Iteration 270, loss = 0.14612498\n",
            "Iteration 271, loss = 0.14725541\n",
            "Iteration 272, loss = 0.14631702\n",
            "Iteration 273, loss = 0.14530805\n",
            "Iteration 274, loss = 0.14700582\n",
            "Iteration 275, loss = 0.14736915\n",
            "Iteration 276, loss = 0.14678328\n",
            "Iteration 277, loss = 0.14607861\n",
            "Iteration 278, loss = 0.14877034\n",
            "Iteration 279, loss = 0.14498870\n",
            "Iteration 280, loss = 0.14546908\n",
            "Iteration 281, loss = 0.14367957\n",
            "Iteration 282, loss = 0.14437343\n",
            "Iteration 283, loss = 0.14553158\n",
            "Iteration 284, loss = 0.14456805\n",
            "Iteration 285, loss = 0.14624899\n",
            "Iteration 286, loss = 0.14388919\n",
            "Iteration 287, loss = 0.14407902\n",
            "Iteration 288, loss = 0.14392343\n",
            "Iteration 289, loss = 0.14558469\n",
            "Iteration 290, loss = 0.14375098\n",
            "Iteration 291, loss = 0.14546641\n",
            "Iteration 292, loss = 0.14233412\n",
            "Iteration 293, loss = 0.14381708\n",
            "Iteration 294, loss = 0.14317935\n",
            "Iteration 295, loss = 0.14377977\n",
            "Iteration 296, loss = 0.14534186\n",
            "Iteration 297, loss = 0.14275980\n",
            "Iteration 298, loss = 0.14358913\n",
            "Iteration 299, loss = 0.14239796\n",
            "Iteration 300, loss = 0.14253690\n",
            "Iteration 301, loss = 0.14197727\n",
            "Iteration 302, loss = 0.14356217\n",
            "Iteration 303, loss = 0.14351066\n",
            "Iteration 304, loss = 0.14156058\n",
            "Iteration 305, loss = 0.14295685\n",
            "Iteration 306, loss = 0.14094540\n",
            "Iteration 307, loss = 0.14151393\n",
            "Iteration 308, loss = 0.14243547\n",
            "Iteration 309, loss = 0.14056657\n",
            "Iteration 310, loss = 0.14030210\n",
            "Iteration 311, loss = 0.14068045\n",
            "Iteration 312, loss = 0.14195968\n",
            "Iteration 313, loss = 0.13989733\n",
            "Iteration 314, loss = 0.13968516\n",
            "Iteration 315, loss = 0.14111471\n",
            "Iteration 316, loss = 0.13989607\n",
            "Iteration 317, loss = 0.14140159\n",
            "Iteration 318, loss = 0.14157079\n",
            "Iteration 319, loss = 0.14115934\n",
            "Iteration 320, loss = 0.14052845\n",
            "Iteration 321, loss = 0.14018298\n",
            "Iteration 322, loss = 0.13914333\n",
            "Iteration 323, loss = 0.13818209\n",
            "Iteration 324, loss = 0.14095192\n",
            "Iteration 325, loss = 0.13936826\n",
            "Iteration 326, loss = 0.14015451\n",
            "Iteration 327, loss = 0.13803627\n",
            "Iteration 328, loss = 0.13930837\n",
            "Iteration 329, loss = 0.14012336\n",
            "Iteration 330, loss = 0.13823972\n",
            "Iteration 331, loss = 0.13842206\n",
            "Iteration 332, loss = 0.13780948\n",
            "Iteration 333, loss = 0.13792718\n",
            "Iteration 334, loss = 0.13871377\n",
            "Iteration 335, loss = 0.13858249\n",
            "Iteration 336, loss = 0.13989538\n",
            "Iteration 337, loss = 0.13717704\n",
            "Iteration 338, loss = 0.13736510\n",
            "Iteration 339, loss = 0.13580214\n",
            "Iteration 340, loss = 0.13642877\n",
            "Iteration 341, loss = 0.13779170\n",
            "Iteration 342, loss = 0.13711757\n",
            "Iteration 343, loss = 0.13669774\n",
            "Iteration 344, loss = 0.13537865\n",
            "Iteration 345, loss = 0.13689178\n",
            "Iteration 346, loss = 0.13765853\n",
            "Iteration 347, loss = 0.13844352\n",
            "Iteration 348, loss = 0.13703501\n",
            "Iteration 349, loss = 0.13557381\n",
            "Iteration 350, loss = 0.13710392\n",
            "Iteration 351, loss = 0.13530963\n",
            "Iteration 352, loss = 0.13593116\n",
            "Iteration 353, loss = 0.13608684\n",
            "Iteration 354, loss = 0.13675924\n",
            "Iteration 355, loss = 0.13467259\n",
            "Iteration 356, loss = 0.13706732\n",
            "Iteration 357, loss = 0.13464525\n",
            "Iteration 358, loss = 0.13467820\n",
            "Iteration 359, loss = 0.13532309\n",
            "Iteration 360, loss = 0.13453114\n",
            "Iteration 361, loss = 0.13460542\n",
            "Iteration 362, loss = 0.13499568\n",
            "Iteration 363, loss = 0.13750787\n",
            "Iteration 364, loss = 0.13765880\n",
            "Iteration 365, loss = 0.13696958\n",
            "Iteration 366, loss = 0.13765098\n",
            "Iteration 367, loss = 0.13596193\n",
            "Iteration 368, loss = 0.13397321\n",
            "Iteration 369, loss = 0.13589362\n",
            "Iteration 370, loss = 0.13358311\n",
            "Iteration 371, loss = 0.13431720\n",
            "Iteration 372, loss = 0.13571534\n",
            "Iteration 373, loss = 0.13404447\n",
            "Iteration 374, loss = 0.13428087\n",
            "Iteration 375, loss = 0.13359165\n",
            "Iteration 376, loss = 0.13284709\n",
            "Iteration 377, loss = 0.13323772\n",
            "Iteration 378, loss = 0.13155805\n",
            "Iteration 379, loss = 0.13386115\n",
            "Iteration 380, loss = 0.13391948\n",
            "Iteration 381, loss = 0.13414555\n",
            "Iteration 382, loss = 0.13354869\n",
            "Iteration 383, loss = 0.13205931\n",
            "Iteration 384, loss = 0.13379987\n",
            "Iteration 385, loss = 0.13218250\n",
            "Iteration 386, loss = 0.13196625\n",
            "Iteration 387, loss = 0.13570356\n",
            "Iteration 388, loss = 0.13218023\n",
            "Iteration 389, loss = 0.13364065\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
              "              verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-10 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-10 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-10 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-10 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-10 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-10 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-10 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-10 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-10 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-10 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-10 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-10 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-10 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-10 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-10 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-10 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
              "              verbose=True)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsao_census = rede_neural_census.predict(X_census_teste)\n",
        "previsao_census"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBG527ULjUaA",
        "outputId": "d3dbd634-2083-40ed-fcd6-ebba93a1d4ed"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' >50K'],\n",
              "      dtype='<U6')"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_census_teste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woCPzZ7ujZbF",
        "outputId": "d5650ab9-eb52-4b15-c633-4192f5a551e6"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' <=50K'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_census_teste, previsao_census)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6UCP0eTjh1d",
        "outputId": "58d8edc9-6795-4f5c-919d-06f5063901c2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.809007164790174"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = ConfusionMatrix(rede_neural_census)\n",
        "cm.fit(X_census_treinamento, y_census_treinamento)\n",
        "cm.score(X_census_teste, y_census_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "G4vfGFWKjoH2",
        "outputId": "ebe1b486-0667-4239-c264-6fa5c370ed4c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.809007164790174"
            ]
          },
          "metadata": {},
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_census_teste, previsao_census))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jltj3-kpjvCT",
        "outputId": "158e5f59-f003-47a2-c4a2-5999f803c5ef"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.87      0.88      0.87      3693\n",
            "        >50K       0.61      0.59      0.60      1192\n",
            "\n",
            "    accuracy                           0.81      4885\n",
            "   macro avg       0.74      0.74      0.74      4885\n",
            "weighted avg       0.81      0.81      0.81      4885\n",
            "\n"
          ]
        }
      ]
    }
  ]
}